{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2PiVfdU1VfP"
   },
   "source": [
    "### Required Setup\n",
    "\n",
    "If you don't have Python on your computer, you can use the [Anaconda Python distribution](http://continuum.io/downloads) to install most of the Python packages you need. Anaconda provides a simple double-click installer for your convenience.\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. The primary libraries that we'll be using are:\n",
    "\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn/sklearn**: The essential Machine Learning package in Python.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **Seaborn**: Advanced statistical plotting library.\n",
    "* **waterqmark**: A Jupyter Notebook extension for printing timestamps, version numbers, and hardware information.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "```\n",
    "conda create -n SYSC4415_tutorials python=3.11\n",
    "conda activate SYSC4415_tutorials\n",
    "\n",
    "conda install jupyter\n",
    "conda install numpy pandas scikit-learn matplotlib seaborn graphviz statsmodels\n",
    "conda install -c conda-forge watermark\n",
    "\n",
    "```\n",
    "\n",
    "`conda` may ask you to update some of them if you don't have the most recent version. Allow it to do so.\n",
    "\n",
    "**Note:** I will not support people trying to run this notebook outside of the Anaconda Python distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2PiVfdU1VfP"
   },
   "source": [
    "# Tutorial 2-3 - Car Prices & Linear Regression\n",
    "\n",
    "**Course:** SYSC 4415 - Introduction to Machine Learning\n",
    "\n",
    "**Semester:** Winter 2026\n",
    "\n",
    "**Adapted by:** [Kevin Dick](https://kevindick.ai/), [Igor Bogdanov](mailto:igorbogdanov@cmail.carleton.ca)\n",
    "\n",
    "**Adapted From:** https://sungsoo.github.io/2018/04/11/predicting-car-prices.html\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we will explore the process of:\n",
    "1. loading tabular data\n",
    "2. examining raw variables\n",
    "3. transforming variables\n",
    "4. exploratory data analysis (EDA)\n",
    "5. building linear regression model (regression)\n",
    "6. building a logistic regression model (classification)\n",
    "\n",
    "This is an example of *predictive analytics* using a dataset that most people can relate to: **prices of a car**!\n",
    "\n",
    "In the data set, we find the following eight variables:\n",
    "\n",
    "* *(Int)* Price \n",
    "* *(Int)* Age \n",
    "* *(Int)* KM (kilometers driven)\n",
    "* *(String)* Fuel Type\n",
    "* *(Int)* HP(horsepower)\n",
    "* *(Bool)* Automatic or Manual\n",
    "* *(Int)* Number of Doors\n",
    "* *(Bool)* Metallic Colour\n",
    "* *(Int)* CC (total volume of air and fuel being pushed through the engine by the cylinders in cubic centimeters)\n",
    "* *(Int)* Weight(in pounds)\n",
    "\n",
    "Data for 1,436 vehicles are collected in a CSV file for a single type of vehicle: the _Toyota Corolla_. You can download this dataset from the original author's github account here: https://github.com/datailluminations/PredictingToyotaPricesBlog\n",
    "\n",
    "### The Target Varriable\n",
    "In **predictive models**, there is a response variable (also called dependent variable or **target variable**), which is the variable that we are interested in predicting.\n",
    "\n",
    "### The Features\n",
    "The **independent variables** (the predictors, also called **features** in the machine learning community) are one or more numeric variables we are using to predict the response variable. Given we are using a linear regression model, we are assuming the relationship between the independent and dependent variables follow a straight line. Later, one could examine more complex models to see if it improves predictive powers.\n",
    "\n",
    "# 1. Loading Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1632068030008,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "idY-PG2J46Yu",
    "outputId": "4ce33b76-26c3-4f35-fd98-b80627948a16"
   },
   "outputs": [],
   "source": [
    "# First load up any packages we will need!\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sb\n",
    "\n",
    "import pandas.testing as tm\n",
    "\n",
    "# to verify what we are using\n",
    "import sys \n",
    "print(sys.executable)  \n",
    "print(sys.version) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9wEiH-q1VfT"
   },
   "source": [
    "Note: If you are missing certain packages, you can use **pip** within the command line to install them!\n",
    "If using Google's Colab, you can use a cell with a leading exclamation point to execute shell commands. For example: \n",
    "`!pip install pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1632068070712,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "cr3S1ooMX-mj",
    "outputId": "8d463165-1257-49d3-a2d1-8e09b1480436"
   },
   "outputs": [],
   "source": [
    "# data source & load into a dataframe\n",
    "file = 'https://raw.githubusercontent.com/datailluminations/PredictingToyotaPricesBlog/master/ToyotaCorolla.csv'\n",
    "car_data = pd.read_csv(file)\n",
    "\n",
    "# set the column headers\n",
    "df = pd.DataFrame(car_data, columns= ['Price', 'Age', 'KM', 'FuelType', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight'])\n",
    "\n",
    "# (optional) unpack columns into individual variables\n",
    "price     = df['Price']\n",
    "age       = df['Age']\n",
    "km        = df['KM']\n",
    "fuelType  = df['FuelType']\n",
    "hp        = df['HP']\n",
    "metColor  = df['MetColor']\n",
    "automatic = df['Automatic']\n",
    "cc        = df['CC']\n",
    "doors     = df['Doors']\n",
    "weight    = df['Weight']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-QNQeUr1VfU"
   },
   "source": [
    "# 2. Examine Raw Variables\n",
    "\n",
    "Before we start the modeling exercise, it’s good to take a visual look at what we are trying to predict to see what it looks like. Since we are **trying to predict Toyota Corolla prices with historical data**, we can create simple histogram plots to examine the distribution of Corolla prices (or another variable of our choosing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD4mm5Hj1VfV",
    "outputId": "4fabd56c-0ce5-4904-bf2d-458e17cfd8e6"
   },
   "outputs": [],
   "source": [
    "# We can also examine the summary statistics for each feature (note that categorical features aren't present)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1632068137503,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "KMgZNN4j1VfV",
    "outputId": "a1f0107c-29f9-46ab-c679-4b945110f68c"
   },
   "outputs": [],
   "source": [
    "# Change this variable to the one we are interested in plotting (why does FuelType throw an error?)\n",
    "var2plot = 'Age' # 'Price', 'KM', 'FuelType', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight'\n",
    "\n",
    "plt.figure(figsize= (6,6))\n",
    "plt.hist(df[var2plot], bins=25, color= 'grey', edgecolor= 'black')\n",
    "plt.xlabel(var2plot)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of Corolla {var2plot}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1632068137503,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "KMgZNN4j1VfV",
    "outputId": "a1f0107c-29f9-46ab-c679-4b945110f68c"
   },
   "outputs": [],
   "source": [
    "# Change this variable to the one we are interested in plotting (why does FuelType throw an error?)\n",
    "var2plot = 'KM' # 'Price', 'Age', 'KM', 'FuelType', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight'\n",
    "\n",
    "plt.figure(figsize= (6,6))\n",
    "plt.hist(df[var2plot], bins=25, color= 'grey', edgecolor= 'black')\n",
    "plt.xlabel(var2plot)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of Corolla {var2plot}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1632068137503,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "KMgZNN4j1VfV",
    "outputId": "a1f0107c-29f9-46ab-c679-4b945110f68c"
   },
   "outputs": [],
   "source": [
    "# Change this variable to the one we are interested in plotting (why does FuelType throw an error?)\n",
    "var2plot = 'FuelType' # 'Price', 'Age', 'KM', 'FuelType', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight'\n",
    "\n",
    "plt.figure(figsize= (6,6))\n",
    "plt.hist(df[var2plot], bins=25, color= 'grey', edgecolor= 'black')\n",
    "plt.xlabel(var2plot)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of Corolla {var2plot}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1632068137503,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "KMgZNN4j1VfV",
    "outputId": "a1f0107c-29f9-46ab-c679-4b945110f68c"
   },
   "outputs": [],
   "source": [
    "# Change this variable to the one we are interested in plotting (why does FuelType throw an error?)\n",
    "var2plot = 'Price' # 'Age', 'KM', 'FuelType', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight'\n",
    "\n",
    "plt.figure(figsize= (6,6))\n",
    "plt.hist(df[var2plot], bins=25, color= 'grey', edgecolor= 'black')\n",
    "plt.xlabel(var2plot)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of Corolla {var2plot}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eHMP_dGjA_E"
   },
   "source": [
    "We see that most used Corollas cost around 10K and there are some at the tail end that over 25K (possibly newer cars).\n",
    "\n",
    "# 3. Data Transformation\n",
    "\n",
    "One of the main steps in the predictive analytics is data transformation. Data is **seldom arranged in the way you want them**. \n",
    "\n",
    "One might have to do some kinds of transformations to get it to the way we need them to be either because the data is dirty, not of the type we want, out of bounds, and a host of other reasons.\n",
    "\n",
    "### Catagorical Variable --> Numerical Variable\n",
    "In this case, we need to convert the categorical variables to numeric variables to feed into our linear regression model, because **linear regression models only take numeric variables**.\n",
    "\n",
    "The categorical variable we want to do the transformation on is **Fuel Types**. We note that there are 3 Fuel Types:\n",
    "1. CNG \n",
    "2. Diesel \n",
    "3. Petrol\n",
    "\n",
    "Let's first count how many there are of each and then encode this into a numerical format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1632068492148,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "rs6xaGL0kEB5",
    "outputId": "c7ba3c88-c695-4569-92f5-f986b2d2abd6"
   },
   "outputs": [],
   "source": [
    "# print out all of the unique fuelType categories\n",
    "print(fuelType.unique())\n",
    "\n",
    "# count the number of instances of each (summation of the subset for each; a value of 1 for each TRUE instance)\n",
    "num_cng    = np.sum(fuelType == \"CNG\")\n",
    "num_diesel = np.sum(fuelType == \"Diesel\")\n",
    "num_petrol = np.sum(fuelType == \"Petrol\")\n",
    "print(f\"CNG:\\t{num_cng}\\nDiesel:\\t{num_diesel}\\nPetrol:\\t{num_petrol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFP20uUzk2MC"
   },
   "source": [
    "So, we can convert the categorical variable Fuel Type to *two numeric variables*: **FuelType1** and **FuelType2**. \n",
    "\n",
    "We assign CNG to a new variable FuelType1 in which a 1 represents it’s a CNG vehicle and 0 it’s not. \n",
    "Likewise, we assign Diesel to a new variable FuelType2 in which a 1 represents it’s a Diesel vehicle and 0 it’s not.\n",
    "\n",
    "So, what do we do with PETROL vehicles? We could have introduced a third varialbe (e.g. FuelType3), but here we will represent PETROL by the case when BOTH FuelType1 and FuelType2 are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1632068498540,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "Vz3x7v9xk7_U",
    "outputId": "6255e2a6-3790-471a-8931-6ce257eac5cf"
   },
   "outputs": [],
   "source": [
    "# convert categorical variables to integer so we can use in our model\n",
    "fuelType1 = (fuelType == \"CNG\")* 1  # multiple by 1 to ensure numeric representation True* 1 = 1, False* 1= 0\n",
    "fuelType2 = (fuelType == \"Diesel\")* 1\n",
    "\n",
    "# Add these new variables to our DataFrame\n",
    "df[\"FuelType1\"] = fuelType1\n",
    "df[\"FuelType2\"] = fuelType2\n",
    "\n",
    "print(df.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYcvFMmLnJp5"
   },
   "source": [
    "# 4. Exploratory Data Analysis (EDA):\n",
    "\n",
    "The next step in predictive analytics is to explore our underlying data structure. \n",
    "\n",
    "Let’s do a few plots of our explantory variables to see how they look against `Price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1472,
     "status": "ok",
     "timestamp": 1632068546148,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "c8ukP03vnQwd",
    "outputId": "2f3bf1fe-cb0d-4d7c-f7bf-319c036f8b8e"
   },
   "outputs": [],
   "source": [
    "# plot variables against price\n",
    "plt.figure(figsize= (12,26))\n",
    "\n",
    "plt.subplot(5, 2, 1); plt.scatter(age, price, s=10); plt.xlabel(\"Age\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. Age\")\n",
    "plt.subplot(5, 2, 2); plt.scatter(km, price, s=10); plt.xlabel(\"KM\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. KM\")\n",
    "plt.subplot(5, 2, 3); plt.scatter(fuelType2, price, s=10); plt.xlabel(\"Not Diesel (0) Diesel (1)\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. Diesel\")\n",
    "plt.subplot(5, 2, 4); plt.scatter(hp, price, s=10); plt.xlabel(\"Horse Power\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. HP\")\n",
    "plt.subplot(5, 2, 5); plt.scatter(cc, price, s=10); plt.xlabel(\"Cubic Centimeters\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. Engine Size\")\n",
    "plt.subplot(5, 2, 6); plt.scatter(doors, price, s=10); plt.xlabel(\"Number of Doors\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. Doors\")\n",
    "plt.subplot(5, 2, 7); plt.scatter(weight, price, s=10); plt.xlabel(\"Weight\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. Weight\")\n",
    "plt.subplot(5, 2, 8); plt.scatter(metColor, price, s=10); plt.xlabel(\"Metallic Color\"); plt.ylabel(\"Price\"); plt.title(\"MetColour vs. Weight\")\n",
    "plt.subplot(5, 2, 9); plt.scatter(automatic, price, s=10); plt.xlabel(\"Manual (0) or Automatic (1)\"); plt.ylabel(\"Price\"); plt.title(\"Price vs. Transmission\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAfUsgAwxCgk"
   },
   "source": [
    "From these plots, we can obtain a few insights: \n",
    "\n",
    "* (Age) **Newer cars** tend to be **more expensive** (as expected)\n",
    "* (KM) The **more miles** a car has the **cheaper** it is.\n",
    "* (HP) Horsepower - this one is not as direct as the other. Yes, the more horsepower the more expensive. But not always the case. Let’s see how this variable will behave in our model.\n",
    "* (Metcolor) The fact that a color has a **Metallic Color** or not **doesn’t seem to be that useful**. But let’s see what the model says...\n",
    "* (automatic) **Transmission type** does not have much of an influence on the **prices**.\n",
    "* (CC) The **engine size** plots against price seems to show the **larger engines** tend to be **more expensive** though not always the case.\n",
    "* (doors) Plotting number of doors does not tell us much.\n",
    "* (weight) The heavier(i.e. bigger) cars cost more though there are some outliers that doesn’t fit nicely.\n",
    "\n",
    "When working on a new problem with tabular data, the insights from Exploratory Data Analysis (EDA) help in building a \"story\" about the data and develop an intuition for how certain variables relate to others.\n",
    "\n",
    "# 5. Model Building - Linear Regression\n",
    "\n",
    "Now that we have explored our variables, let’s perform a simple linear regression of `Price` against all the data we’ve collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1632068629242,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "a5Btj-wVxuvz",
    "outputId": "c928c618-7636-4165-f9b6-fe2edc572d27"
   },
   "outputs": [],
   "source": [
    "# setup foor a linear regression (we MUST remove the dependent variable from the features)\n",
    "x  = df[['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight', 'FuelType1', 'FuelType2']] # features/independent variables\n",
    "x  = sm.add_constant(x)   # Recall that sm is 'statsmodels.api'\n",
    "y1 = df['Price']        # our dependent variable\n",
    "\n",
    "# fit our linear regression model\n",
    "model1 = sm.OLS(y1, x).fit()   # using Ordinary Least Squares \n",
    "\n",
    "# apply trained/learned model back to training data\n",
    "pred1 = model1.predict(x)  \n",
    "\n",
    "# print summary stats from model fit\n",
    "print(model1.summary())  \n",
    "\n",
    "# The predictions are listed in pred1 in order\n",
    "print(pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WIZMcNSx9Bu"
   },
   "source": [
    "### Model Interpretation\n",
    "\n",
    "We see from the output that our model captures **86.9% (see Multiple R square) of the variation** in `price` using the explantory variables above. This is pretty decent.\n",
    "\n",
    "However, we notice is that **some coefficents are more statistically significant than others**. \n",
    "\n",
    "For example, we find that `Age` is the **most significant** witha t-value of -46.889, followed by `Weight` with a t-value of 16.629. \n",
    "\n",
    "The **least significant variables** are `Metallic Color` and `Number of Doors`. This was also confirmed in our EDA (exploratory data analysis) graphs above.\n",
    "\n",
    "### Correcting the Data Split\n",
    "Now, it’s generally **NOT a good idea to use your ENTIRE data sample to fit the model**. \n",
    "\n",
    "What we want to do is to **train the model on a sample of the data**. \n",
    "Then we’ll **see how it perform outside of our training sample**. \n",
    "\n",
    "This breaking up of our data set into a **training set** and **test set** to evaluate the performance of our models with _unseen data_. Using the entire data set to build a model then using the entire data set to evaluate how good a model does is a bit of cheating or careless analytics.\n",
    "\n",
    "### Results with Training Data:\n",
    "\n",
    "Here we will use the first 1,000 rows (of 1,436 iinstances; ~2/3) as our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1632068710529,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "FrVtCSidvj5O",
    "outputId": "e9ebde82-6375-4a87-f8ca-df502a453472"
   },
   "outputs": [],
   "source": [
    "x  = df[['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight', 'FuelType1', 'FuelType2']]\n",
    "y1 = df['Price']\n",
    "\n",
    "# take first 1000 samples of independent variables as training set\n",
    "trainX = x[0:1000]  \n",
    "trainX = sm.add_constant(trainX) # add constant\n",
    "trainY = y1[0:1000] # take first 1000 samples of price as well as training set\n",
    "\n",
    "# Train model on training data\n",
    "model2 = sm.OLS(trainY, trainX).fit()\n",
    "trainingResults = model2.predict(trainX) # Evaluate model on training set\n",
    " \n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDjwzHb2yN2b"
   },
   "source": [
    "### Model Interpretation\n",
    "\n",
    "Interesting enough, the **R-squared only changed nominally to 86.4%** and the variables **t-value also moved slightly**. The **statistically significant relationships remained the same**. Good1\n",
    "\n",
    "### Model Evaluation: Linear Regression\n",
    "\n",
    "The real test of a good model is to **test the model with data that it has not been fitted**. Here’s where the rubber meets the road. We apply our model to unseen data to see how it performs.\n",
    "\n",
    "### Prediction using out-of-sample data\n",
    "\n",
    "Here are some common metrics to see how well the model predicts using various error metrics. \n",
    "The main takeway is we **want our forecast errors to be as small as possible**. The smaller the forecast error the better the model is at predicting unseen data.\n",
    "\n",
    "#### Metric 1: Mean Absolute Error (MAE)\n",
    "The first metric is **mean absolute error (MAE)**: the average absolute value of the error observed over all test points. The smaller the better!\n",
    "\n",
    "$\\left ( \\frac{1}{n} \\right) \\sum_{i=1}^{n}\\left | y_{i} - \\hat{y_{i}}\\right |$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1632068739432,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "L2LZpLkE6OTp",
    "outputId": "339b17a5-08fc-4b40-c397-bf4f1cf11212"
   },
   "outputs": [],
   "source": [
    "testX= x[1000:]  # take the remaining samples of independent variables for testing set\n",
    "testX = sm.add_constant(testX) # add constant\n",
    "testY= y1[1000:] # take the remaining samples of price as well for testing set\n",
    "\n",
    "testingResults = model2.predict(testX) # results of trained model on new data\n",
    "#print(testingResults)\n",
    "\n",
    "# mean absolute error\n",
    "mae = sm.tools.eval_measures.meanabs(testY, testingResults)\n",
    "print(\"mean absolute error: %.2f\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LT8Al5xyayG"
   },
   "source": [
    "The **ideal MAE is zero**, which means on average the **predicted value perfectly matches the actual value**. This is rarely if ever the case. \n",
    "\n",
    "As in all things, we must determine what is an acceptable level of errors for our predictive analytics model and accept it. No such thing as a perfect model!\n",
    "\n",
    "#### Metric 2: Root Mean Squared Error (RMSE)\n",
    "The second metric is **root mean squared error (RMSE)**: the average of the squared differences between the predicted value and the actual value. \n",
    "\n",
    "The reason we square is to not account for sign differences(negative differences and positive differences are the same thing when squared). RMSE brings it back to our normal unit by taking the square root of MSE.\n",
    "\n",
    "$ \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big( y_i - \\hat{y_i}\\Big)^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1632068750778,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "a1FVRG5LychT",
    "outputId": "7620e6f5-9330-4175-b4bc-96f597b2e873"
   },
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "rmse= sm.tools.eval_measures.rmse(testY, testingResults)\n",
    "print(\"root mean squared error: %.2f\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2u3CKDdydY9"
   },
   "source": [
    "#### Metric 3: Mean Absolute Percent Error\n",
    "\n",
    "The third metric is **mean absolute percent error (MAPE)**: expresses the forecasted errors as a percentage. \n",
    "\n",
    "$\\displaystyle\\frac{100\\%}{n}\\sum_{i=1}^{n}\\left |\\frac{y_i - \\hat{y_i}}{y_ii}\\right|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1632068762438,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "HNboT5U1ygDF",
    "outputId": "f13e3a65-4bab-4a27-f062-cb604b3aee99"
   },
   "outputs": [],
   "source": [
    "# mean absolute percent error\n",
    "mape= np.mean(np.abs((testY - testingResults) / testY)) * 100\n",
    "print('mean absolute percent error: %.2f' % mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsWFRdx-yhDq"
   },
   "source": [
    "On average, our model had a forecast error of only 12.2%. Not bad for a first pass at this data set.\n",
    "\n",
    "# 6. Model Building - Logistic Regression\n",
    "\n",
    "Now let's say we want to use this data to create a model that will **predict whether or not we can afford a car** from the variables we have discussed. \n",
    "\n",
    "**Assume:** We have 10,000 dollars to spend. \n",
    "\n",
    "We **use the budget of 10,000 dollars as a trheshold criteria for assigning cars a class**:\n",
    "1. (class 1) we can afford the car (<= 10,000) \n",
    "2. (class 0) we cannot afford the car\n",
    "\n",
    "Let's prepare the data for this **binary classification** task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1632068777467,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "O1YtTDxdylzZ",
    "outputId": "48e15680-bd87-4883-de65-d24ed0e63782"
   },
   "outputs": [],
   "source": [
    "# logistic regression for afford/ cannot afford\n",
    "# we are building dataset to use for training\n",
    "\n",
    "df2 = df.drop(columns=['FuelType']) # make a copy of df to use in logistic regression (get rid of the non-numeric FuelType)\n",
    "cash_on_hand = 10000 # what we can afford\n",
    "\n",
    "#price is the variable that stores columns from before\n",
    "\n",
    "price2 = (price <= cash_on_hand) * 1 # create array of logical class labels based on whether or not we can afford\n",
    "\n",
    "df2[\"Price\"] = price2 # update price with class labels\n",
    "\n",
    "print(df2.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXzi5y7Da270"
   },
   "source": [
    "Great, now we have a binary classification problem! \n",
    "Let's start by plotting how many of each class there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1632068800995,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "hNk-Uwp8bK09",
    "outputId": "ccedf808-b88f-40a1-d6ec-26a7d24604c4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (6,6))\n",
    "plt.hist(price2, bins=2, color= 'grey', edgecolor= 'black'); plt.xlabel(\"Cannot afford (0) Can afford (1)\"); plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEvw8pgDbce3"
   },
   "source": [
    "Okay, great - these two classes look relatively equal in size. \n",
    "\n",
    "### Class Balance\n",
    "Visualizing the class distribution is a good first step, because you can check for class imbalance and get an indication of how to read your classifier's accuracy. **If our two classes were split 50/50 (0/1) and we scored a classification accuracy of 50%, then we know that our classifier is no better than a coin toss (random chance)**. \n",
    "\n",
    "If the classes were split 90/10 (0/1) and we got the same accuracy, then we would know that our classifier was better than random.\n",
    "\n",
    "### Visualizing Features\n",
    "\n",
    "Now let's look at our data again. When we visualized our data in the linear regression model, we were **looking for variables that formed a _linear trend_ with price**. \n",
    "\n",
    "Now in the logistic regression, we are **looking for features that _separate our two classes_ into two groups**. The greater the separation of the distributions, the better the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fwyW9MhfdGkn",
    "outputId": "3ce3171a-9116-4a0b-d659-f19b20b7ac0f"
   },
   "outputs": [],
   "source": [
    "# ['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight']\n",
    "# ['Age', 'KM', 'HP', ]\n",
    "\n",
    "df2_scater1 = df2.drop(columns=['MetColor', 'Automatic', 'CC', 'Doors', 'Weight', 'FuelType1', 'FuelType2'])\n",
    "\n",
    "# Pair plots are an effectiive way to visualize all combinatioins of features\n",
    "sb.pairplot(df2_scater1, hue='Price', height= 5, diag_kws={'bw_method': 0.9})\n",
    "plt.show()\n",
    "# NOTE (F20): statsmodels==0.11.1,seaborn==0.10.0, requires specification of a float value for the KDE bandwidth for the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fwyW9MhfdGkn",
    "outputId": "3ce3171a-9116-4a0b-d659-f19b20b7ac0f"
   },
   "outputs": [],
   "source": [
    "# ['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight']\n",
    "# ['Age', 'KM', 'HP', ]\n",
    "df2_scater2 = df2.drop(columns=['Age', 'KM', 'HP', 'FuelType1', 'FuelType2'])\n",
    "\n",
    "# Pair plots are an effectiive way to visualize all combinatioins of features\n",
    "sb.pairplot(df2_scater2, hue='Price', height= 5, diag_kws={'bw_method': 0.9})\n",
    "plt.show()\n",
    "# NOTE (F20): statsmodels==0.11.1,seaborn==0.10.0, requires specification of a float value for the KDE bandwidth for the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fwyW9MhfdGkn",
    "outputId": "3ce3171a-9116-4a0b-d659-f19b20b7ac0f"
   },
   "outputs": [],
   "source": [
    "# ['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight']\n",
    "# ['Age', 'KM', 'HP', ]\n",
    "df2_scater2 = df2.drop(columns=['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight'])\n",
    "\n",
    "# Pair plots are an effectiive way to visualize all combinatioins of features\n",
    "sb.pairplot(df2_scater2, hue='Price', height= 5, diag_kws={'bw_method': 0.9})\n",
    "plt.show()\n",
    "# NOTE (F20): statsmodels==0.11.1,seaborn==0.10.0, requires specification of a float value for the KDE bandwidth for the diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOb-V1YYfE9U"
   },
   "source": [
    "These plots tell us that cars in class 1 tend to be: \n",
    "\n",
    "* older,\n",
    "* have higher km,\n",
    "* and have lower horsepower.\n",
    "\n",
    "These plots also tell us, as they did in the linear regression, that `metallic color`, `transmission type`, `number of cyclinders`, `number of doors`, `weight`, and `fuel type` do not distinguish these two classes very well.\n",
    "\n",
    "### Creating a Logistic Regression Model\n",
    "\n",
    "Let's see what we get when we build a logistic regression classifier using these features. We'll start by making a **training data set using 70% of our data** and a **test data set using the remaining 30%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1632068957217,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "G5xXdpF7t7pr",
    "outputId": "fc366cd2-ae6f-4463-9ccd-b5548660651a"
   },
   "outputs": [],
   "source": [
    "x = df2[['Age', 'KM', 'HP', 'MetColor', 'Automatic', 'CC', 'Doors', 'Weight', 'FuelType1', 'FuelType2']]\n",
    "y = df2['Price']\n",
    "\n",
    "# Partition training and testing data sets\n",
    "samples    = x.shape[0]       # returns number of samples\n",
    "szTraining = int(samples*0.7) # 70% rounded to the nearest integer\n",
    "trainX     = x[0:szTraining]\n",
    "trainY     = y[0:szTraining]\n",
    "testX      = x[szTraining:]\n",
    "testY      = y[szTraining:]\n",
    "\n",
    "# Construct model using all features\n",
    "logitModel = sm.Logit(trainY,trainX)\n",
    "result     = logitModel.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sylItKn2DB0f"
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "As we discussed earlier in our data visualizations, **some features are more important than others for separating our two classes**. We want to separate the good features from the bad features here. \n",
    "\n",
    "Good features make our model better, while bad features add noise that may hinder classification accuracy. \n",
    "\n",
    "There are many algorithms for removing bad features that we will discuss later in the course, but for now we will use the *p*-values in the table above. Don't worry about the details of *p*-values; we will cover them in more detail later. All we need to know now is that features with **smaller *p*-values (<= 0.05) are features are useful for predicting class**, and these are the ones we want to keep in our model.\n",
    "\n",
    "`Age`, `KM`, `Automatic`, `CC`, `Doors`, `Weight`, and `FuelType1` all have low p-values and will be kept. The rest will be discarded from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1632069025412,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "cA3R3q3WiUuG",
    "outputId": "1ceb9ee0-20a2-43d6-8179-d2c548614cc3"
   },
   "outputs": [],
   "source": [
    "# Select only important features to use in model\n",
    "trainX2    = trainX[['Age', 'KM', 'Automatic', 'CC', 'Doors', 'Weight', 'FuelType1']]\n",
    "logitModel = sm.Logit(trainY,trainX2) \n",
    "result     = logitModel.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZGaPvcHt56X"
   },
   "source": [
    "### Model Interpretation\n",
    "With the other noisy features removed, the **_p_-values for** `CC` **and** `FuelType1` **have shot up**. Let's remove these now and train our final model on the remaining features. \n",
    "\n",
    "We will then move to testing the model on our test data set and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1632069079416,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "KZMC3OqRGyiB",
    "outputId": "674c6b63-d090-461b-81a7-a54ebb4a51dd"
   },
   "outputs": [],
   "source": [
    "# Trim more poor features and train final model\n",
    "trainX2 = trainX[['Age', 'KM', 'Automatic', 'Doors', 'Weight']]\n",
    "testX2  = testX[['Age', 'KM', 'Automatic', 'Doors', 'Weight']]\n",
    "\n",
    "logitModel = sm.Logit(trainY, trainX2)\n",
    "result     = logitModel.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1632069084169,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 240
    },
    "id": "NB2gkLk61Vfg",
    "outputId": "cbef94b1-955c-43e7-da07-eb2e3928b693"
   },
   "outputs": [],
   "source": [
    "predY = result.predict(testX2).round()\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(accuracy_score(testY, predY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHjutyscHlr-"
   },
   "source": [
    "We have made a classifier that can **correctly predict whether or not we can afford a car 97% of the time!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Kevin Dick, Igor Bogdanov' -nmv --packages numpy,pandas,sklearn,matplotlib,seaborn,graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_eUxvSK1Vfg"
   },
   "source": [
    "# Take Away Messages\n",
    "\n",
    "* In **predictive models**, there is a response variable (also called dependent variable or **target variable**).\n",
    "* The **independent variables** (a.k.a. **features**) are one or more numeric variables we are using to predict the response variable. \n",
    "* Before modeling, it’s good to **take a visual look** at what we are trying to predict to see what it looks like.\n",
    "* Data is **seldom arranged in the way you want them**; we need to convert the categorical variables to numeric variables.\n",
    "* With tabular data, the insights from EDA help in building a \"story\" about the data and develop an intuition for how certain variables relate to others.\n",
    "* Some coefficents are **more statistically significant** than others. \n",
    "* It’s  **NOT a good idea to use your ENTIRE data sample to fit the model**; use a train/test split. \n",
    "* Train the model on a sample of the data then **see how it perform outside of our training sample**. \n",
    "* Metric 1: **mean absolute error (MAE)**: the average absolute value of the error observed over all test points. The smaller the better!\n",
    "$\\left ( \\frac{1}{n} \\right) \\sum_{i=1}^{n}\\left | y_{i} - \\hat{y_{i}}\\right |$\n",
    "* Metric 2: **root mean squared error (RMSE)**: the average of the squared differences between the predicted value and the actual value. \n",
    "$ \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big( y_i - \\hat{y_i}\\Big)^2}}$\n",
    "* Metric 3: **mean absolute percent error (MAPE)**: expresses the forecasted errors as a percentage. \n",
    "$\\displaystyle\\frac{100\\%}{n}\\sum_{i=1}^{n}\\left |\\frac{y_i - \\hat{y_i}}{y_ii}\\right|$\n",
    "* Visualizing the class distribution is a good first step, because you can check for class imbalance and get an indication of how to read your classifier's accuracy. \n",
    "* If our two classes were **split 50/50 (0/1)** and we scored a classification **accuracy of 50%**, then we know that our **classifier is no better than a coin toss (random chance)**. \n",
    "* Linear Regression: we were **looking for variables that formed a _linear trend_ with price**. \n",
    "* Logistic Regression: we are **looking for features that _separate our two classes_ into two groups** (greater separation of a distribution, the better the feature).\n",
    "* **Some features are more important than others for separating our two classes**. We want to separate the good features from the bad features here. \n",
    "* **Smaller *p*-values (<= 0.05) are features are useful for predicting class**, and these are the ones we want to keep in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0ZKdAf21Vfg"
   },
   "source": [
    "# That's all folks!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tutorial-2_CarsLinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
